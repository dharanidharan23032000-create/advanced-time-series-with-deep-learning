Abstract

Time series forecasting is essential in industrial environments where continuous sensor readings are used to predict system behavior and support decision making. This work demonstrates a complete forecasting pipeline using a deep learning approach. A multivariate synthetic dataset representing industrial sensor behavior was generated with clear trend, seasonal variations, and noise. A Long Short-Term Memory (LSTM) model was trained to predict the future value of one sensor based on a window of historical measurements from multiple sensors. Feature-level explainability was incorporated through SHAP to identify the contribution of each sensor to the forecast. The resulting model achieved effective accuracy based on multiple evaluation metrics and provided clear interpretability through SHAP feature importance
Dataset Description

The dataset was programmatically generated to represent realistic industrial sensors. Five sensors were simulated using a combination of increasing trend, sinusoidal seasonal components with different frequencies, and Gaussian noise to represent disturbances. The target variable was defined as the next value of sensor_1, shifted one step ahead. This formulation reflects a practical scenario where previous sensor readings are used to predict short-term behavior. Generating synthetic data allows control over the statistical properties of the signal, ensures multivariate characteristics, and avoids common limitations of raw real-world sensor data such as missing values, sampling noise, and irregular collection.
Pre-processing and Windowing

The sensor features were normalized using standard scaling to improve model stability and training convergence. Since LSTM models require sequential inputs, a sliding window approach was applied. Each training example consisted of the previous thirty time steps of all five sensors, forming a supervised learning dataset where the input shape is a three-dimensional tensor containing samples, time steps, and features. The data was separated chronologically into training, validation, and test sets in a 70:15:15 ratio to avoid information leakage across time. This ensures that evaluation reflects the true performance on future unseen data rather than memorized past values.
LSTM Model Architecture

The forecasting architecture was based on a stacked LSTM network. The model consisted of two LSTM layers with sixty-four hidden units each, followed by a dense output layer that mapped the last hidden representation of the sequence into a single numerical prediction. The stacked architecture improves the model’s ability to capture both short- and long-term dependencies in the sensor patterns. Each training batch contained sequences of shape (batch size, 30, 5), where the last time step representation was passed to the final layer. The Adam optimizer with a learning rate of 0.001 was used for training, and mean squared error was selected as the loss function due to its suitability for continuous numerical forecasting.
Hyperparameter Optimization

Hyperparameters significantly influence forecasting accuracy. A targeted search strategy was applied to evaluate multiple configurations by varying the number of LSTM layers, the dimensionality of hidden units, the learning rate, and the batch size. Evaluation of each configuration was performed using validation data, and the best-performing model was chosen based on the lowest mean absolute error. The final selected configuration consisted of two LSTM layers with sixty-four hidden units, a batch size of thirty-two, and a learning rate of 0.001. This configuration was then trained using the combined training and validation dataset to enhance generalization.
Experimental Results

The optimized model was tested on the held-out dataset and evaluated using multiple metrics. The performance achieved was: mean absolute error (MAE) = 1.1784, root mean squared error (RMSE) = 1.4943, and mean absolute percentage error (MAPE) = 6.12%. These values indicate that the model captured both the trend and seasonal fluctuations in the sensor signal while maintaining a relatively small percentage deviation. The prediction visualization showed that the model closely tracked the overall behavior of the true values, although rapid spikes caused by noise were naturally smoothed due to the temporal averaging behavior of LSTM representations.
Explainability Using SHAP

Explainability was introduced through SHAP, allowing the model’s forecasting logic to be evaluated at a feature level. SHAP values measure the contribution of each sensor to the final prediction while keeping the model unchanged. The analysis demonstrated that sensor_1 had the highest importance, which was expected since the model predicts its future value. Moderate influence was observed from sensor_2 and sensor_4, suggesting relationships driven by shared seasonal patterns. The remaining sensors contributed auxiliary information with weaker magnitude. This selective feature influence confirms that the model did not rely equally on all sensors, and instead learned meaningful dependencies.
Conclusion

The study presented a complete deep learning workflow for multivariate time series forecasting with interpretability. A realistic synthetic dataset allowed full control over sensor characteristics and ensured stable experimentation. The optimized LSTM model demonstrated reliable forecasting capability with low error, and the integration of SHAP provided valuable transparency by identifying how individual sensor features affected predictions. Combining forecasting performance with explainability enhances trust and supports potential application in industrial analytics and predictive maintenance, where both accuracy and interpretability are critical.
